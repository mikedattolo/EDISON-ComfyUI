# EDISON-ComfyUI

<div align="center">

<img src="logo.png" alt="EDISON Logo" width="200"/>

**Your Complete Offline AI Platform**

*Enterprise-grade local LLM system with modern web UI, vision capabilities, RAG memory, and ComfyUI integration*

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Ubuntu 22.04+](https://img.shields.io/badge/ubuntu-22.04%2B-orange.svg)](https://ubuntu.com/)
[![CUDA 12+](https://img.shields.io/badge/CUDA-12%2B-green.svg)](https://developer.nvidia.com/cuda-downloads)

[Features](#features) ‚Ä¢ [Quick Start](#quick-start) ‚Ä¢ [Documentation](#documentation) ‚Ä¢ [Troubleshooting](#troubleshooting)

</div>

---

## üéØ Why EDISON?

**EDISON** (Enhanced Distributed Intelligence System for Offline Networks) is a production-ready, fully offline AI platform that brings enterprise-level capabilities to your local infrastructure:

- ‚úÖ **100% Private** - All processing happens on your hardware
- ‚úÖ **No API Costs** - One-time setup, unlimited usage
- ‚úÖ **Multi-Modal** - Text, images, code, and vision understanding
- ‚úÖ **Production Ready** - Systemd services, logging, monitoring
- ‚úÖ **Multi-GPU Support** - Efficient tensor splitting across GPUs
- ‚úÖ **Memory System** - RAG-powered long-term context retention

---

## ‚ú® Features

### üöÄ **Modern Web Interface**
- **Claude/ChatGPT-inspired UI** with real-time streaming responses
- **Conversation Management** - Save, load, and organize chat histories
- **File Upload Support** - Attach documents and images to conversations
- **Multiple AI Modes** - Chat, Deep Thinking, Reasoning, Code, Agent
- **Auto-Intent Detection** - Automatically selects optimal model and mode
- **Hardware Monitoring** - Real-time GPU, CPU, and memory stats
- **Work Mode** - Dedicated interface for complex multi-step tasks
- **Settings Panel** - Configure models, memory, and behavior

### üß† **Powerful AI Models**
- **Fast Mode (Qwen 2.5 14B)** - Instant responses for quick questions
- **Deep Mode (Qwen 2.5 72B)** - Comprehensive analysis and reasoning
- **Vision Model (LLaVA 1.6)** - Image understanding and description
- **Code Assistant** - Specialized for programming and technical tasks
- **Agent Mode** - Web search, tool use, and multi-step problem solving
- **Full GPU Acceleration** - CUDA-optimized inference with tensor splitting

### üé® **Image Generation**
- **ComfyUI Integration** - Professional node-based workflow system
- **Custom EDISON Nodes** - Direct chat interface within ComfyUI
- **Workflow Library** - Pre-configured templates for common tasks
- **Real-time Generation** - Streaming progress updates

### üß© **Advanced Capabilities**
- **RAG Memory System** - Vector-based long-term context with Qdrant
- **Web Search** - Agent mode with DuckDuckGo integration
- **File Processing** - Upload and analyze documents, images, and code
- **Intent Classification** - Optional Google Coral TPU acceleration
- **Multi-GPU Support** - Automatic load balancing across devices
- **Conversation Memory** - Remember facts, preferences, and context
- **Smart Prompting** - Context-aware system prompts per mode

### üîå **Enterprise Deployment**
- **One-Command Install** - Automated setup with dependency management
- **Systemd Services** - Auto-start on boot with crash recovery
- **Production Logging** - Structured logs with journalctl integration
- **Health Monitoring** - Endpoints for uptime and status checks
- **Network Access** - Secure LAN-accessible from any device
- **Zero Configuration** - Intelligent defaults with optional customization

---

## üöÄ Quick Start

### Prerequisites
- Ubuntu 22.04+ or Debian-based Linux
- NVIDIA GPU with 12GB+ VRAM (16GB+ recommended)
- CUDA 12.0+ drivers installed
- 100GB+ free disk space for models

### Installation (5 minutes)

```bash
# 1. Clone repository
git clone https://github.com/mikedattolo/EDISON-ComfyUI.git
cd EDISON-ComfyUI

# 2. Check system requirements
./scripts/check_system.sh

# 3. Run automated installation (installs all dependencies)
./scripts/setup_ubuntu.sh

# 4. Download AI models (~50GB)
python3 scripts/download_models.py

# 5. Install as system services (auto-start on boot)
sudo ./scripts/enable_services.sh

# 6. Access web interface
# Open browser: http://YOUR_SERVER_IP:8080
```

### What Gets Installed

| Service | Port | Description | Auto-Start |
|---------|------|-------------|------------|
| **edison-web** | 8080 | Modern web UI for chat | ‚úÖ Yes |
| **edison-core** | 8811 | LLM inference API | ‚úÖ Yes |
| **edison-coral** | 8808 | Intent classification | ‚úÖ Yes |
| **edison-comfyui** | 8188 | ComfyUI image generation | ‚úÖ Yes |

### First Steps

1. **Open Web UI** - Navigate to `http://YOUR_IP:8080`
2. **Try Chat Mode** - Ask: "What can you help me with?"
3. **Test Deep Mode** - Ask: "Explain quantum computing in detail"
4. **Upload Files** - Click üìé to attach documents or images
5. **Try Vision** - Upload an image and ask "What's in this image?"
6. **Enable Memory** - Toggle "Remember" to store conversations

---

## üìä EDISON vs Cloud AI

| Feature | EDISON | ChatGPT/Claude | Local Alternatives |
|---------|--------|----------------|-------------------|
| **Privacy** | 100% offline | Cloud-based | Varies |
| **Cost** | One-time hardware | $20-200/month | Varies |
| **Latency** | <1s local | 1-5s network | <1s local |
| **Customization** | Full control | Limited | Limited |
| **Multi-modal** | Text + Vision + Image Gen | Text + Vision | Often separate |
| **Memory/RAG** | Built-in Qdrant | Token-limited | Often missing |
| **Production Ready** | Systemd services | N/A | Manual setup |
| **Multi-GPU** | Tensor splitting | N/A | Rarely supported |
| **Web Interface** | Modern, responsive | Excellent | Basic |
| **API Access** | OpenAI-compatible | Yes | Varies |

---

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Web Browser (Any Device)           ‚îÇ
‚îÇ            http://YOUR_IP:8080                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ                    ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  EDISON Web  ‚îÇ   ‚îÇ    ComfyUI      ‚îÇ
‚îÇ   (8080)     ‚îÇ   ‚îÇ    (8188)       ‚îÇ
‚îÇ  - Chat UI   ‚îÇ   ‚îÇ  - Node Editor  ‚îÇ
‚îÇ  - File Up   ‚îÇ   ‚îÇ  - Workflows    ‚îÇ
‚îÇ  - Settings  ‚îÇ   ‚îÇ  - Generation   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                    ‚îÇ
       ‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ           ‚îÇ EDISON Nodes    ‚îÇ
       ‚îÇ           ‚îÇ (Custom Nodes)  ‚îÇ
       ‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                    ‚îÇ
       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       EDISON Core (8811)            ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ   LLM Inference Engine      ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  - Qwen 2.5 14B (Fast)      ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  - Qwen 2.5 72B (Deep)      ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  - LLaVA 1.6 (Vision)       ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  - llama-cpp-python + CUDA  ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ               ‚îÇ                     ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ   RAG System (Qdrant)       ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  - Vector embeddings        ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  - Conversation memory      ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  - Document storage         ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ               ‚îÇ                     ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ   Tool System               ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  - Web Search (DuckDuckGo)  ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  - Code execution           ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   EDISON Coral (8808)                ‚îÇ
‚îÇ   Intent Classification              ‚îÇ
‚îÇ   - Auto mode detection             ‚îÇ
‚îÇ   - Coral TPU acceleration (opt)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üíª System Requirements

### Minimum Configuration
| Component | Requirement | Notes |
|-----------|-------------|-------|
| **OS** | Ubuntu 22.04+ | Debian-based Linux |
| **CPU** | 8+ cores | Intel/AMD x86_64 |
| **RAM** | 32GB | For 14B models |
| **Storage** | 100GB free | For models + system |
| **GPU** | 12GB VRAM | RTX 3060 12GB minimum |
| **Python** | 3.9+ | Installed by setup script |

### Recommended Configuration
| Component | Requirement | Notes |
|-----------|-------------|-------|
| **RAM** | 64GB+ | For 72B deep model |
| **Storage** | 200GB+ NVMe | Faster model loading |
| **GPU** | 16GB+ VRAM | RTX 4080/4090/A5000 |
| **CUDA** | 12.0+ | Latest drivers |
| **Network** | Gigabit LAN | For remote access |
| **Optional** | Coral M.2 TPU | Intent classification |

### Multi-GPU Support
EDISON automatically splits models across multiple GPUs:
- **3x GPU Setup**: 50% / 25% / 25% tensor split
- **2x GPU Setup**: 60% / 40% tensor split  
- **1x GPU**: Full model on single GPU

---

## Installation

### Automated Installation (Recommended)

```bash
# Clone repository
git clone https://github.com/mikedattolo/EDISON-ComfyUI.git
cd EDISON-ComfyUI

# Run system check
./scripts/check_system.sh

# Install everything
./scripts/setup_ubuntu.sh

# Download models
python3 scripts/download_models.py

# Deploy as services
sudo ./scripts/enable_services.sh
```

### Services Installed

After installation, the following services will be running:

| Service | Port | Description |
|---------|------|-------------|
| `edison-web` | 8080 | Modern web UI for chat interface |
| `edison-core` | 8811 | Main AI service with LLM inference |
| `edison-coral` | 8808 | Intent classification service |
| `edison-comfyui` | 8188 | ComfyUI for image generation |

All services auto-start on boot and are accessible from network.

---

## Usage

### Web UI

Open your browser to **http://YOUR_IP:8080**

Features:
- üí¨ **Chat modes**: Auto, Chat (14B), Deep (72B), Code, Agent
- üìù **Conversation history** saved in browser
- ‚öôÔ∏è **Settings panel** for configuration
- üîç **System status** monitoring
- üé® **Modern dark theme**

### API Usage

#### Chat Endpoint

```bash
curl -X POST http://YOUR_IP:8811/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Explain quantum computing",
    "mode": "deep",
    "remember": true
  }'
```

#### Intent Classification

```bash
curl -X POST http://YOUR_IP:8808/intent \
  -H "Content-Type: application/json" \
  -d '{"text": "Generate an image of a sunset"}'
```

#### Health Check

```bash
curl http://YOUR_IP:8811/health
curl http://YOUR_IP:8080/health
```

### ComfyUI Integration

1. Open ComfyUI at **http://YOUR_IP:8188**
2. Right-click ‚Üí **Add Node** ‚Üí **EDISON** ‚Üí **EDISON Chat**
3. Configure:
   - **text**: Your message
   - **mode**: auto, chat, deep, code, or agent
   - **remember**: true/false
   - **timeout_seconds**: 120 (default)

### Modes Explained

| Mode | Model | Use Case |
|------|-------|----------|
| **Auto** | Detected | Automatically selects best mode based on intent |
| **Chat** | 14B | Quick responses, casual conversation |
| **Deep** | 72B | Detailed analysis, complex reasoning |
| **Code** | 72B | Programming assistance, debugging |
| **Agent** | 72B | Tool-using, multi-step tasks |

---

## Service Management

```bash
# Check status
sudo systemctl status edison-core edison-web

# Restart services
sudo systemctl restart edison-core

# View logs
sudo journalctl -u edison-core -f

# Stop services
sudo systemctl stop edison-core edison-web

# Disable auto-start
sudo systemctl disable edison-core
```

---

## Documentation

üìö **Comprehensive guides available:**

- **[INSTALL.md](INSTALL.md)** - Detailed installation guide with all options
- **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)** - Solutions to common issues
- **[COPILOT_SPEC.md](COPILOT_SPEC.md)** - Original specification
- **[IMPLEMENTATION_COMPLETE.md](IMPLEMENTATION_COMPLETE.md)** - Implementation notes

---

## Troubleshooting

### Common Issues

**‚ùå Disk space full**
```bash
# Expand LVM partition
sudo lvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv
sudo resize2fs /dev/ubuntu-vg/ubuntu-lv
```

**‚ùå Python 3.12 + Coral TPU**
- Coral `pycoral` library incompatible with Python 3.12
- System automatically uses heuristic classification instead
- Or use separate Python 3.11 venv for coral service

**‚ùå Service won't start**
```bash
# Check logs for specific error
sudo journalctl -u edison-core -n 50

# Common fix: Ensure models are downloaded
ls -lh /opt/edison/models/llm/
```

**‚ùå Can't access from network**
```bash
# Check firewall
sudo ufw allow 8080/tcp
sudo ufw allow 8811/tcp
sudo ufw allow 8188/tcp
```

**‚ùå Coral TPU kernel module errors**
```bash
# Automated fix for kernel 6.8+ API changes
sudo ./scripts/fix_coral_tpu.sh
sudo reboot
```

**‚ùå GPU not working**
```bash
# Verify CUDA
nvidia-smi
nvcc --version

# Check PyTorch GPU support
python3 -c "import torch; print(torch.cuda.is_available())"
```

See **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)** for detailed solutions.

---

## Updating

```bash
cd /workspaces/EDISON-ComfyUI
git pull
source .venv/bin/activate
pip install -r requirements.txt --upgrade

# If services installed:
sudo ./scripts/enable_services.sh
sudo systemctl daemon-reload
sudo systemctl restart edison-core edison-web edison-comfyui
```

---

## Uninstallation

```bash
# Stop and remove services
sudo systemctl stop edison-coral edison-core edison-web edison-comfyui
sudo systemctl disable edison-coral edison-core edison-web edison-comfyui
sudo rm /etc/systemd/system/edison-*.service
sudo systemctl daemon-reload

# Remove installation
sudo rm -rf /opt/edison

# Remove user (optional)
sudo userdel edison
```

---

## Development

### Local Development Mode

Run services locally without systemd:

```bash
# Terminal 1: Coral service
source .venv/bin/activate
python -m uvicorn services.edison_coral.service:app --host 0.0.0.0 --port 8808

# Terminal 2: Core service
source .venv/bin/activate
python -m uvicorn services.edison_core.app:app --host 0.0.0.0 --port 8811

# Terminal 3: Web UI
source .venv/bin/activate
python -m uvicorn services.edison_web.service:app --host 0.0.0.0 --port 8080

# Terminal 4: ComfyUI
source .venv/bin/activate
cd ComfyUI
python main.py --listen 0.0.0.0 --port 8188
```

### Project Structure

```
EDISON-ComfyUI/
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ edison_core/          # Main AI service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py           # FastAPI app with LLM inference
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts.py       # System prompts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag.py           # RAG with Qdrant
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tools.py         # Agent tools
‚îÇ   ‚îú‚îÄ‚îÄ edison_coral/        # Intent classification
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ service.py
‚îÇ   ‚îú‚îÄ‚îÄ edison_web/          # Web UI service
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ service.py
‚îÇ   ‚îî‚îÄ‚îÄ systemd/             # Service files
‚îú‚îÄ‚îÄ web/                     # Web UI frontend
‚îÇ   ‚îú‚îÄ‚îÄ index.html          # Main HTML
‚îÇ   ‚îú‚îÄ‚îÄ styles.css          # Modern styling
‚îÇ   ‚îî‚îÄ‚îÄ app.js              # Client-side logic
‚îú‚îÄ‚îÄ ComfyUI/
‚îÇ   ‚îî‚îÄ‚îÄ custom_nodes/
‚îÇ       ‚îî‚îÄ‚îÄ edison_nodes/    # EDISON custom nodes
‚îú‚îÄ‚îÄ scripts/                 # Installation scripts
‚îÇ   ‚îú‚îÄ‚îÄ check_system.sh     # System requirements checker
‚îÇ   ‚îú‚îÄ‚îÄ setup_ubuntu.sh     # Main setup script
‚îÇ   ‚îú‚îÄ‚îÄ fix_coral_tpu.sh    # Coral TPU auto-fixer
‚îÇ   ‚îú‚îÄ‚îÄ download_models.py  # Model downloader
‚îÇ   ‚îî‚îÄ‚îÄ enable_services.sh  # Service installer
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ edison.yaml         # Main configuration
‚îÇ   ‚îî‚îÄ‚îÄ gpu_map.yaml        # GPU assignments
‚îî‚îÄ‚îÄ models/                  # Model storage
    ‚îú‚îÄ‚îÄ llm/                # GGUF models
    ‚îú‚îÄ‚îÄ qdrant/             # Vector database
    ‚îî‚îÄ‚îÄ embeddings/         # Sentence transformers
```

---

## Performance Tips

### For 14B Model Only
If you only have 32GB RAM, use only the fast model:
```yaml
# config/edison.yaml
edison:
  core:
    fast_model: "qwen2.5-14b-instruct-q4_k_m.gguf"
    deep_model: "qwen2.5-14b-instruct-q4_k_m.gguf"  # Same as fast
```

### GPU Optimization
```python
# services/edison_core/app.py
llm_fast = Llama(
    model_path=str(fast_model_path),
    n_ctx=4096,
    n_gpu_layers=-1,      # -1 = offload all layers to GPU
    n_threads=8,          # CPU threads for non-GPU work
    verbose=False
)
```

### Multiple GPUs
```bash
# Set in systemd service
Environment="CUDA_VISIBLE_DEVICES=0,1,2"
```

---

## Security Considerations

### Network Exposure
By default, all services bind to `0.0.0.0` for network access. For localhost-only:

```bash
# Edit service files
sudo nano /etc/systemd/system/edison-web.service
# Change: --host 0.0.0.0 to --host 127.0.0.1
sudo systemctl daemon-reload
sudo systemctl restart edison-web
```

### Firewall
```bash
# Allow specific IPs only
sudo ufw default deny incoming
sudo ufw allow from 192.168.1.0/24 to any port 8080
sudo ufw allow from 192.168.1.0/24 to any port 8811
sudo ufw enable
```

---

## Known Issues

### Python 3.12 + Coral TPU
- **Issue**: `pycoral` library not compatible with Python 3.12
- **Status**: System automatically uses heuristic classification
- **Workaround**: Use separate Python 3.11 venv for coral service
- **Tracking**: Waiting for pycoral Python 3.12 support

### RTX 5060 Ti (Blackwell)
- **Issue**: PyTorch 2.5.1 doesn't support compute capability 12.0
- **Status**: GPU falls back to CPU
- **Workaround**: Use other GPUs or wait for PyTorch 2.6+
- **Tracking**: PyTorch 2.6 will add Blackwell support

### Coral TPU Kernel 6.8+
- **Issue**: gasket driver incompatible with kernel 6.8+ API changes
- **Status**: ‚úÖ Fixed with automated patcher
- **Solution**: Run `sudo ./scripts/fix_coral_tpu.sh`

---

## Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

Areas for contribution:
- Additional model support
- UI improvements
- Documentation
- Bug fixes
- Performance optimizations

---

## License

MIT License - see [LICENSE](LICENSE) file for details

---

## Acknowledgments

- **llama-cpp-python** - Fast LLM inference
- **ComfyUI** - Node-based image generation
- **Qwen** - Excellent open-source models
- **Qdrant** - Vector database
- **FastAPI** - Modern web framework
- **Google Coral** - Edge TPU acceleration

---

## Support

- üìñ [Installation Guide](INSTALL.md)
- üîß [Troubleshooting Guide](TROUBLESHOOTING.md)
- üêõ [Issue Tracker](https://github.com/mikedattolo/EDISON-ComfyUI/issues)
- üí¨ [Discussions](https://github.com/mikedattolo/EDISON-ComfyUI/discussions)

---

<div align="center">

**Made with ‚ù§Ô∏è for the open-source AI community**

[‚¨Ü Back to Top](#edison-comfyui)

</div>

# Activate venv
source .venv/bin/activate

# Terminal 1 - Coral service
python -m uvicorn services.edison_coral.service:app --host 127.0.0.1 --port 8808

# Terminal 2 - Core service
python -m uvicorn services.edison_core.app:app --host 127.0.0.1 --port 8811

# Terminal 3 - ComfyUI
cd ComfyUI
python main.py --listen 0.0.0.0 --port 8188
```

## Configuration

Edit `/opt/edison/config/edison.yaml`:

```yaml
edison:
  core:
    host: "127.0.0.1"
    port: 8811
    models_path: "models/llm"
    fast_model: "qwen2.5-14b-instruct-q4_k_m.gguf"
    deep_model: "qwen2.5-72b-instruct-q4_k_m.gguf"
  
  coral:
    host: "127.0.0.1"
    port: 8808
```

Restart services after changes:
```bash
sudo systemctl restart edison-core edison-coral
```

## Project Structure

```
EDISON-ComfyUI/
‚îú‚îÄ‚îÄ config/              # Configuration files
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ edison_core/     # LLM service (FastAPI)
‚îÇ   ‚îú‚îÄ‚îÄ edison_coral/    # Intent service (FastAPI)
‚îÇ   ‚îî‚îÄ‚îÄ systemd/         # Service unit files
‚îú‚îÄ‚îÄ scripts/             # Setup and maintenance scripts
‚îú‚îÄ‚îÄ models/              # Model storage
‚îÇ   ‚îú‚îÄ‚îÄ llm/            # GGUF models
‚îÇ   ‚îú‚îÄ‚îÄ qdrant/         # Vector DB
‚îÇ   ‚îî‚îÄ‚îÄ coral/          # Edge TPU models
‚îî‚îÄ‚îÄ ComfyUI/
    ‚îî‚îÄ‚îÄ custom_nodes/
        ‚îî‚îÄ‚îÄ edison_nodes/  # EDISON custom node
```

## License

See LICENSE file.

## Support

For issues, please check:
1. Run `bash scripts/doctor.sh`
2. Check service logs
3. Review COPILOT_SPEC.md for detailed requirements